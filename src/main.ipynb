{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#84b6f4;\">Detección de comentarios de hate</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#77dd77;\">Formación del grafo</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, sacamos los datos del fichero txt y formamos un DataFrame con ellos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      id                                               text   \n",
      "0  id=828025263321657348  Ismael es egocentrico porque se vuelve loca si...  \\\n",
      "1  id=828025128797741057  ..ya tardaba en salir quien pronunciase nombre...   \n",
      "2  id=828025087815274496  (Esto no es un discurso político y razonado, o...   \n",
      "3  id=828025006626058241  Muy despreciados,siiii,pero todos vestidos de ...   \n",
      "4  id=828024709761658880  marica explicame porque a veces no te entiendo...   \n",
      "\n",
      "   hate  \n",
      "0     0  \n",
      "1     0  \n",
      "2     0  \n",
      "3     1  \n",
      "4     1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Especificamos el archivo de texto\n",
    "txt_file = os.getcwd()+'/resources/labeled_corpus_6K.txt'\n",
    "\n",
    "# Leemos el archivo de texto con el formato específico\n",
    "data = []\n",
    "with open(txt_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(';||;')\n",
    "        if len(parts) == 3:\n",
    "            data.append(parts)\n",
    "\n",
    "# Creamos un DataFrame a partir de los datos\n",
    "column_names = ['id', 'text', 'hate']\n",
    "data_frame = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "# Convertimos la columna 'hate' en enteros\n",
    "data_frame['hate'] = data_frame['hate'].astype(int)\n",
    "\n",
    "# Mostramos las primeras filas del DataFrame\n",
    "print(data_frame.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#77dd77;\">Entrenamiento</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos las métricas que usaremos para predecir, así como el atributo objetivo que, en este caso, será numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atributo que usaremos para predecir\n",
    "attributes = data_frame[['text']]\n",
    "\n",
    "# Atributo objetivo a predecir\n",
    "goal = data_frame['hate']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, entrenamos un modelo de clasificación con una red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo en el conjunto de prueba: 0.7744444444444445\n",
      "[[1176  157]\n",
      " [ 249  218]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "text = data_frame['text']\n",
    "\n",
    "# Dividimos los datos en conjuntos de entrenamiento (70%) y prueba (30%)\n",
    "attributes_train, attributes_test, goal_train, goal_test = train_test_split(text, goal, test_size=0.3, random_state=42)\n",
    "\n",
    "# Creamos un vectorizador de texto\n",
    "vectorizer = CountVectorizer()\n",
    "tweet_train_vectorized = vectorizer.fit_transform(attributes_train)\n",
    "tweet_test_vectorized = vectorizer.transform(attributes_test)\n",
    "\n",
    "# Entrenamos el modelo de clasificación de redes neuronales\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(50, 50, 50), activation='relu', solver='adam', random_state=42, max_iter=1000)\n",
    "mlp_classifier.fit(tweet_train_vectorized, goal_train)\n",
    "\n",
    "# Realizamos predicciones con el conjunto de prueba\n",
    "prediction = mlp_classifier.predict(tweet_test_vectorized)\n",
    "\n",
    "# Calculamos la precisión del modelo\n",
    "accuracy = accuracy_score(goal_test, prediction)\n",
    "print(\"Precisión del modelo en el conjunto de prueba:\", accuracy)\n",
    "\n",
    "# Mostramos la matriz de confusión\n",
    "print(confusion_matrix(goal_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar, probamos el modelo con algunos ejemplos concretos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El comentario 'No estoy de acuerdo con tu análisis, creo que la película es muy entretenida.' es respetuoso.\n",
      "El comentario 'Hay que ser idiota para hacer esta reseña.' es de hate.\n",
      "El comentario '¡Enhorabuena por tu trabajo!' es respetuoso.\n",
      "El comentario 'Tu trabajo es basura y tú también.' es de hate.\n"
     ]
    }
   ],
   "source": [
    "message = [\"respetuoso\", \"de hate\"]\n",
    "\n",
    "# Texto de ejemplo 1\n",
    "example_text_1 = \"No estoy de acuerdo con tu análisis, creo que la película es muy entretenida.\"\n",
    "example_text_1_vectorized = vectorizer.transform([example_text_1])\n",
    "\n",
    "# Texto de ejemplo 2\n",
    "example_text_2 = \"Hay que ser idiota para hacer esta reseña.\"\n",
    "example_text_2_vectorized = vectorizer.transform([example_text_2])\n",
    "\n",
    "# Texto de ejemplo 3\n",
    "example_text_3 = \"¡Enhorabuena por tu trabajo!\"\n",
    "example_text_3_vectorized = vectorizer.transform([example_text_3])\n",
    "\n",
    "# Texto de ejemplo 4\n",
    "example_text_4 = \"Tu trabajo es basura y tú también.\"\n",
    "example_text_4_vectorized = vectorizer.transform([example_text_4])\n",
    "\n",
    "print(\"El comentario '\" + example_text_1 + \"' es \" + message[mlp_classifier.predict(example_text_1_vectorized)[0]] + \".\")\n",
    "print(\"El comentario '\" + example_text_2 + \"' es \" + message[mlp_classifier.predict(example_text_2_vectorized)[0]] + \".\")\n",
    "print(\"El comentario '\" + example_text_3 + \"' es \" + message[mlp_classifier.predict(example_text_3_vectorized)[0]] + \".\")\n",
    "print(\"El comentario '\" + example_text_4 + \"' es \" + message[mlp_classifier.predict(example_text_4_vectorized)[0]] + \".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
